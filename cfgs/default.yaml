dataset: MikiV/SimpleStories-SimpleStories-chunked-128

model_shape:
  layers: 2
  d_model: 32
  n_heads: 2
  d_vocab: 4096

# Training hyperparameters
batch_size: 64
batches: 1000
seq_len: 128

gradient_accumulation_steps: 1
learning_rate: 1e-3
lr_warmup_steps: 100
weight_decay: 0.01
eval_interval: 100

validation_batch_size: 1024
validation_batches: 10

# Wandb configuration
wandb_entity: miki-and-tml
wandb_project: scaling-tests
