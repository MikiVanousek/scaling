{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a4d2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237bdba9df054abaa20a90dfaae18ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a26902dd42401f808f07ac9c9efb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a243b67d3c4130903537bee142c4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 4136391\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 41774\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from doclang_scaling.tokenizer import fast_tokenizer\n",
    "import datasets\n",
    "dataset = datasets.load_dataset(\"MikiV/SimpleStories-SimpleStories-chunked-512\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa666bd5-d05b-4b05-87f7-74d95c319834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Eagerly, a girl named Kim went hiking with her friends. They found a secret passageway under a big rock. Curious, they crawled through it and entered a magical world where all the animals talked. They learned that the animals had their own struggles but always found a way to be happy. \\n\\nInspired, the children played with the animals and shared their own stories of hardship. They realized that laughter could help them through hard times. When they finally left, they took the animals' smiles with them, knowin\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab one example from the training set\n",
    "ids = dataset[\"train\"][0][\"input_ids\"]\n",
    "\n",
    "# decode\n",
    "decoded = fast_tokenizer.decode(ids, skip_special_tokens=False)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59b439ac-a9c7-4a92-9d4a-949a68ba90de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e9f97cab80405db2f614df09ecc8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The warm only by a fleeling trees.\" The woman said touches each ento the sun was love. \n",
      "\n",
      "They flower tho\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# Add the parent directory to sys. path\n",
    "sys.path.append(os.path.dirname(('../doclang_scaling')))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Import your model class\n",
    "from doclang_scaling.alibi_transformer import AlibiTransformer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelFiles:\n",
    "    config_path: str\n",
    "    weights_path: str\n",
    "\n",
    "def resolve_model_files(\n",
    "    model_id: Optional[str] = None,\n",
    "    local_model_dir: Optional[str] = None,\n",
    ") -> ModelFiles:\n",
    "    \"\"\"\n",
    "    Returns paths to config.json and pytorch_model.bin either from HF Hub or local folder.\n",
    "    \"\"\"\n",
    "    if model_id:\n",
    "        repo_dir = snapshot_download(repo_id=model_id, allow_patterns=[\"config.json\", \"pytorch_model.bin\"])\n",
    "        config_path = os.path.join(repo_dir, \"config.json\")\n",
    "        weights_path = os.path.join(repo_dir, \"pytorch_model.bin\")\n",
    "    elif local_model_dir:\n",
    "        config_path = os.path.join(local_model_dir, \"config.json\")\n",
    "        weights_path = os.path.join(local_model_dir, \"pytorch_model.bin\")\n",
    "    else:\n",
    "        raise ValueError(\"Provide either model_id (Hub) or local_model_dir (disk).\")\n",
    "    if not os.path.exists(config_path) or not os.path.exists(weights_path):\n",
    "        raise FileNotFoundError(\"Could not find config.json or pytorch_model.bin.\")\n",
    "    return ModelFiles(config_path, weights_path)\n",
    "\n",
    "def load_model(model_files: ModelFiles, device: Optional[torch.device] = None) -> AlibiTransformer:\n",
    "    with open(model_files.config_path, \"r\") as f:\n",
    "        cfg = json.load(f)\n",
    "\n",
    "    # The training script saved:\n",
    "    # {\n",
    "    #   \"model_shape\": {\n",
    "    #       \"layers\": ...,\n",
    "    #       \"d_model\": ...,\n",
    "    #       \"n_heads\": ...,\n",
    "    #       \"d_vocab\": ...,\n",
    "    #       \"ffw_size\": ...,\n",
    "    #       \"d_head\": ...\n",
    "    #   },\n",
    "    #   \"context_length\": ...\n",
    "    # }\n",
    "    ms = cfg[\"model_shape\"]\n",
    "    model = AlibiTransformer(\n",
    "        d_vocab=ms[\"d_vocab\"],\n",
    "        d_model=ms[\"d_model\"],\n",
    "        n_heads=ms[\"n_heads\"],\n",
    "        d_head=ms[\"d_head\"],\n",
    "        ffw_size=ms[\"ffw_size\"],\n",
    "        layers=ms[\"layers\"],\n",
    "        dropout=0.0,\n",
    "    )\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    sd = torch.load(model_files.weights_path, map_location=device)\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    model: AlibiTransformer,\n",
    "    input_ids: torch.Tensor,\n",
    "    max_new_tokens: int = 50,\n",
    "    eos_token_id: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Simple autoregressive generation.\n",
    "    input_ids: [1, T] tensor with tokenized prompt.\n",
    "    Returns: [1, T + max_new_tokens] or early-stop on EOS.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    out = input_ids.to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(out)[:, -1, :]  # [1, vocab]\n",
    "        logits = logits / max(temperature, 1e-6)\n",
    "\n",
    "        if top_k is not None and top_k > 0:\n",
    "            topk_vals, topk_idx = torch.topk(logits, k=top_k, dim=-1)\n",
    "            mask = torch.full_like(logits, fill_value=float(\"-inf\"))\n",
    "            mask.scatter_(dim=-1, index=topk_idx, src=topk_vals)\n",
    "            logits = mask\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # [1, 1]\n",
    "        out = torch.cat([out, next_token], dim=1)\n",
    "\n",
    "        if eos_token_id is not None and next_token.item() == eos_token_id:\n",
    "            break\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# 1) Point to your pushed model on the Hub (preferred) or local folder\n",
    "model_id = \"MikiV/1M-5\"  # e.g., \"myuser/cfgs-default.yaml\"\n",
    "local_model_dir = None  # or \"/tmp/doclang_model_12345\" if running locally\n",
    "\n",
    "model_files = resolve_model_files(model_id=model_id, local_model_dir=local_model_dir)\n",
    "model = load_model(model_files)\n",
    "\n",
    "# 2) Load or recreate the tokenizer you used for training\n",
    "# If uploaded to Hub alongside the model:\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#\n",
    "# Otherwise, recreate it exactly (example below commented):\n",
    "tokenizer = fast_tokenizer\n",
    "\n",
    "# --- Example prompt ---\n",
    "prompt_text = \"The \"\n",
    "# input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids\n",
    "# If you have a simple char-level tokenizer, convert manually:\n",
    "input_ids = torch.tensor([[fast_tokenizer.convert_tokens_to_ids(ch) for ch in prompt_text]], dtype=torch.long)\n",
    "\n",
    "# 3) Generate\n",
    "eos_token_id = getattr(getattr(tokenizer, \"eos_token_id\", None), \"__int__\", lambda: None)()\n",
    "out_ids = generate(\n",
    "    model,\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    eos_token_id=eos_token_id,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# 4) Decode\n",
    "text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "print(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297ebdd-a0b6-4f47-a5ca-8f1ec7a5e78d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
