{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c18c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90903ca9",
   "metadata": {},
   "source": [
    "## Create Chunked Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "539bab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv('.envrc')\n",
    "if 'HF_TOKEN' in os.environ:\n",
    "    login(token=os.environ['HF_TOKEN'])\n",
    "else:\n",
    "    login(token=getpass.getpass('Huggingface token: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa9713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 128\n",
    "SUBCHUNK_SIZE = 64\n",
    "assert CHUNK_SIZE % SUBCHUNK_SIZE == 0\n",
    "SUBCHUNK_RATIO = CHUNK_SIZE // SUBCHUNK_SIZE\n",
    "\n",
    "LARGE_CHUNKED_DATASET = 'MikiV/SimpleStories-SimpleStories-chunked-128'\n",
    "SUBCHUNK_DATASET_NAME = LARGE_CHUNKED_DATASET.replace('chunked', 'subchunked').replace(str(CHUNK_SIZE), f'{SUBCHUNK_SIZE}x{SUBCHUNK_RATIO}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c514e1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1282f746c174afcbcdd0caaf59454f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/390 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15584d19be1c4f0a82deb6ad281b2216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00004.parquet:   0%|          | 0.00/176M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5009266abad04ee59643292004514fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00004.parquet:   0%|          | 0.00/175M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d77b3454e3648f28152e60b421b442e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00004.parquet:   0%|          | 0.00/175M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025aa20cb26d4b7cb0ae423c82e71d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00004.parquet:   0%|          | 0.00/176M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fa573cc4714ccfa9cf6f4dfe37894d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/29.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1c115af7b441b0ad3e677c2f60ab4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3653646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68985882e3524d35a65686cd4762b179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/152426 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 3653646\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 152426\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_dataset = load_dataset(LARGE_CHUNKED_DATASET)\n",
    "chunked_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24751850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539928c57a56409f834420923aacefbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3653646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def split_into_subchunks(example):\n",
    "    # `example` is a single example (not a batch) when batched=False\n",
    "    text = example['input_ids']\n",
    "    subchunks = [\n",
    "        {'input_ids': text[i*SUBCHUNK_SIZE:(i+1)*SUBCHUNK_SIZE]} for i in range(SUBCHUNK_RATIO)\n",
    "    ]\n",
    "    # return a plain list of subchunks so iteration is simple\n",
    "    return {'subchunks': subchunks}\n",
    "\n",
    "# map with batched=False so each mapped item is a dict (not lists),\n",
    "# avoiding the issue where mapping with batched=True can return strings in some versions\n",
    "subchunked_train = chunked_dataset['train'].map(split_into_subchunks, batched=False, remove_columns=chunked_dataset['train'].column_names, num_proc=4)\n",
    "\n",
    "train_data = []\n",
    "for item in subchunked_train:\n",
    "    # each `item` is a dict with key 'subchunks' holding a list of dicts\n",
    "    train_data.extend(item['subchunks'])\n",
    "\n",
    "# create a Dataset directly from the list of dicts\n",
    "subchunked_dataset = DatasetDict({\n",
    "    'train': Dataset.from_list(train_data),\n",
    "    'test': chunked_dataset['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8579c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 7307658 samples\n",
      "Original train set size: 3653829 samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set size: {len(subchunked_dataset['train'])} samples\")\n",
    "print(f\"Original train set size: {len(chunked_dataset['train'])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16783760",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All datasets in `DatasetDict` should have the same features but features for 'train' and 'test' don't match: {'input_ids': List(Value('int32'))} != {'input_ids': List(Value('int32')), 'token_type_ids': List(Value('int8')), 'attention_mask': List(Value('int8'))}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msubchunked_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSUBCHUNK_DATASET_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/epfl/scaling/.pixi/envs/default/lib/python3.13/site-packages/datasets/dataset_dict.py:1730\u001b[39m, in \u001b[36mDatasetDict.push_to_hub\u001b[39m\u001b[34m(self, repo_id, config_name, set_default, data_dir, commit_message, commit_description, private, token, revision, create_pr, max_shard_size, num_shards, embed_external_files, num_proc)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease provide one `num_shards` per dataset in the dataset dictionary, e.g. \u001b[39m\u001b[33m{{\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: 128, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: 4}}\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28mself\u001b[39m._check_values_type()\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_values_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1731\u001b[39m total_uploaded_size = \u001b[32m0\u001b[39m\n\u001b[32m   1732\u001b[39m total_dataset_nbytes = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/epfl/scaling/.pixi/envs/default/lib/python3.13/site-packages/datasets/dataset_dict.py:70\u001b[39m, in \u001b[36mDatasetDict._check_values_features\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item_a, item_b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(items[:-\u001b[32m1\u001b[39m], items[\u001b[32m1\u001b[39m:]):\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m item_a[\u001b[32m1\u001b[39m].features != item_b[\u001b[32m1\u001b[39m].features:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     71\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAll datasets in `DatasetDict` should have the same features but features for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_a[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_b[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_a[\u001b[32m1\u001b[39m].features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_b[\u001b[32m1\u001b[39m].features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: All datasets in `DatasetDict` should have the same features but features for 'train' and 'test' don't match: {'input_ids': List(Value('int32'))} != {'input_ids': List(Value('int32')), 'token_type_ids': List(Value('int8')), 'attention_mask': List(Value('int8'))}"
     ]
    }
   ],
   "source": [
    "subchunked_dataset.push_to_hub(SUBCHUNK_DATASET_NAME, private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c533525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
