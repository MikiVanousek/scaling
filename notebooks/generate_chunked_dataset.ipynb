{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c18c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90903ca9",
   "metadata": {},
   "source": [
    "## Create Chunked Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539bab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv('.envrc')\n",
    "if 'HF_TOKEN' in os.environ:\n",
    "    login(token=os.environ['HF_TOKEN'])\n",
    "else:\n",
    "    login(token=getpass.getpass('Huggingface token: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa9713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_REPO = 'roneneldan/TinyStories'\n",
    "# STORY_KEY = 'text'\n",
    "DATASET_REPO = 'SimpleStories/SimpleStories'\n",
    "STORY_KEY = 'story'\n",
    "\n",
    "TOKENIZER_MODEL = 'SimpleStories/SimpleStories-35M'\n",
    "VALIDATION_SPLIT_PERCENTAGE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c514e1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 2115696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(DATASET_REPO, split='train')\n",
    "print('Total rows:', len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a84943d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 2115696\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the dataset  \n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[STORY_KEY], truncation=False, padding='do_not_pad')\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names).select_columns(['input_ids'])\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9115080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa44b865df4440d9f95af3f305a183c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2031068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CHUNK_LENGTH = 64\n",
    "\n",
    "train_dataset, test_dataset = tokenized_dataset.train_test_split(\n",
    "    test_size=VALIDATION_SPLIT_PERCENTAGE / 100,\n",
    "    seed=42,\n",
    ").values()\n",
    "\n",
    "def chunk_dataset_with_map(dataset, token_column='input_ids', chunk_length=CHUNK_LENGTH):\n",
    "    \"\"\"\n",
    "    Memory-efficient version using dataset.map() and batched processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def chunk_batch(batch):\n",
    "        chunked_batch = {col: [] for col in batch.keys()}\n",
    "        \n",
    "        for i in range(len(batch[token_column])):\n",
    "            tokens = batch[token_column][i]\n",
    "            token_count = len(tokens)\n",
    "            num_chunks = token_count // chunk_length\n",
    "            \n",
    "            for chunk_idx in range(num_chunks):\n",
    "                start_idx = chunk_idx * chunk_length\n",
    "                end_idx = start_idx + chunk_length\n",
    "                chunk_tokens = tokens[start_idx:end_idx]\n",
    "                \n",
    "                # Add chunk to batch\n",
    "                for col in batch.keys():\n",
    "                    if col == token_column:\n",
    "                        chunked_batch[col].append(chunk_tokens)\n",
    "                    else:\n",
    "                        chunked_batch[col].append(batch[col][i])\n",
    "        \n",
    "        return chunked_batch\n",
    "    \n",
    "    # Use map with batched=True for memory efficiency\n",
    "    chunked_dataset = dataset.map(\n",
    "        chunk_batch,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,  # Remove original columns\n",
    "        batch_size=1000,  # Adjust based on your memory constraints\n",
    "    )\n",
    "    \n",
    "    return chunked_dataset\n",
    "chunked_dataset_train = chunk_dataset_with_map(train_dataset, chunk_length=CHUNK_LENGTH)\n",
    "chunked_dataset_test = chunk_dataset_with_map(test_dataset, chunk_length=CHUNK_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24751850",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = datasets.DatasetDict({\n",
    "    'train': chunked_dataset_train,\n",
    "    'test': chunked_dataset_test\n",
    "})\n",
    "split_dataset.push_to_hub(f\"MikiV/{DATASET_REPO.replace('/', '-')}-chunked-{CHUNK_LENGTH}\", private=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1359d8",
   "metadata": {},
   "source": [
    "## Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbcfeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historgram of tokenized lengths\n",
    "lengths = [len(x) for x in tokenized_dataset['input_ids']]\n",
    "plt.hist(lengths, bins=50)\n",
    "plt.xlabel('Tokenized Length')\n",
    "plt.ylabel('Number of Stories')\n",
    "plt.title(f'Histogram of Tokenized Lengths in {DATASET_REPO} Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b407fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens_train = len(chunked_dataset_train)*CHUNK_LENGTH/1e6\n",
    "total_tokens_test = len(chunked_dataset_test)*CHUNK_LENGTH/1e6\n",
    "total_tokens_original = sum(lengths)/1e6\n",
    "print(f\"Total available tokens = {total_tokens_original}M tokens\")\n",
    "print(f\"Total tokens in chunked dataset (train) = {total_tokens_train}M tokens\")\n",
    "print(f\"Total tokens in chunked dataset (test) = {total_tokens_test}M tokens\")\n",
    "print(f\"We are left with {(total_tokens_train + total_tokens_test)/total_tokens_original * 100}% of the original tokens (train)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae51852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thresh in [64, 128, 256, 512]:\n",
    "    x = sum(l >= thresh for l in lengths) / len(lengths) * 100\n",
    "    print(f\"{x}% of stories are at least {thresh} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda4a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ove chunked dataset, find the most common tokens and what percentage of the dataset they cover\n",
    "from collections import Counter\n",
    "all_tokens = [token for chunk in chunked_dataset['input_ids'] for token in chunk]\n",
    "token_counts = Counter(all_tokens)\n",
    "total_tokens = len(all_tokens)\n",
    "most_common = token_counts.most_common(20)\n",
    "for token, count in most_common:\n",
    "    print(f\"Token: {token} ({tokenizer.decode([token])}), Count: {count}, Percentage: {count/total_tokens*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f87f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
