{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "616f494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc895fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvanousekmikulas\u001b[0m (\u001b[33mvanousekmikulas-epfl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/tmp/ipykernel_772450/3146412732.py:35: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  runs_df = pd.concat([pd.DataFrame([row]), runs_df]).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>run_name</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_loss_ci_lower</th>\n",
       "      <th>val_loss_ci_upper</th>\n",
       "      <th>tokens_seen</th>\n",
       "      <th>compute</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xl2usnof</td>\n",
       "      <td>model_6.8M_tokens_120.0M</td>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.032532</td>\n",
       "      <td>0.032760</td>\n",
       "      <td>119996416</td>\n",
       "      <td>979201473642496</td>\n",
       "      <td>6840320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vwci9mym</td>\n",
       "      <td>model_6.8M_tokens_60.0M</td>\n",
       "      <td>0.044834</td>\n",
       "      <td>0.044665</td>\n",
       "      <td>0.044998</td>\n",
       "      <td>59998208</td>\n",
       "      <td>489600736821248</td>\n",
       "      <td>6840320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agrcb37f</td>\n",
       "      <td>model_6.8M_tokens_30.0M</td>\n",
       "      <td>0.252477</td>\n",
       "      <td>0.251721</td>\n",
       "      <td>0.253218</td>\n",
       "      <td>29999104</td>\n",
       "      <td>244800368410624</td>\n",
       "      <td>6840320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2zf1ghwn</td>\n",
       "      <td>model_6.8M_tokens_240.0M</td>\n",
       "      <td>0.028772</td>\n",
       "      <td>0.028660</td>\n",
       "      <td>0.028878</td>\n",
       "      <td>239992832</td>\n",
       "      <td>1958402947284992</td>\n",
       "      <td>6840320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h09i8eaz</td>\n",
       "      <td>model_6.8M_tokens_480.0M</td>\n",
       "      <td>0.025872</td>\n",
       "      <td>0.025769</td>\n",
       "      <td>0.025980</td>\n",
       "      <td>479993856</td>\n",
       "      <td>3916872743387136</td>\n",
       "      <td>6840320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>buol3dn1</td>\n",
       "      <td>model_13.8M_tokens_960.0M</td>\n",
       "      <td>0.021329</td>\n",
       "      <td>0.021235</td>\n",
       "      <td>0.021425</td>\n",
       "      <td>959995904</td>\n",
       "      <td>14015632999710720</td>\n",
       "      <td>13797376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bh0nkk09</td>\n",
       "      <td>model_13.8M_tokens_480.0M</td>\n",
       "      <td>0.023663</td>\n",
       "      <td>0.023562</td>\n",
       "      <td>0.023761</td>\n",
       "      <td>479993856</td>\n",
       "      <td>7007756699566080</td>\n",
       "      <td>13797376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i8ikrs6s</td>\n",
       "      <td>model_13.8M_tokens_240.0M</td>\n",
       "      <td>0.026442</td>\n",
       "      <td>0.026337</td>\n",
       "      <td>0.026546</td>\n",
       "      <td>239992832</td>\n",
       "      <td>3503818549493760</td>\n",
       "      <td>13797376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>g7yc8q47</td>\n",
       "      <td>model_13.8M_tokens_120.0M</td>\n",
       "      <td>0.029614</td>\n",
       "      <td>0.029507</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>119996416</td>\n",
       "      <td>1751909274746880</td>\n",
       "      <td>13797376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27l1qho0</td>\n",
       "      <td>model_13.8M_tokens_60.0M</td>\n",
       "      <td>0.034794</td>\n",
       "      <td>0.034669</td>\n",
       "      <td>0.034935</td>\n",
       "      <td>59998208</td>\n",
       "      <td>875954637373440</td>\n",
       "      <td>13797376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i052nchb</td>\n",
       "      <td>model_1.8M_tokens_108.0M</td>\n",
       "      <td>0.168284</td>\n",
       "      <td>0.167679</td>\n",
       "      <td>0.168922</td>\n",
       "      <td>107995136</td>\n",
       "      <td>312961264517120</td>\n",
       "      <td>1846016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>inzap0mx</td>\n",
       "      <td>model_1.8M_tokens_54.0M</td>\n",
       "      <td>1.129691</td>\n",
       "      <td>1.127677</td>\n",
       "      <td>1.131667</td>\n",
       "      <td>53993472</td>\n",
       "      <td>156468762378240</td>\n",
       "      <td>1846016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1n8ous25</td>\n",
       "      <td>model_1.8M_tokens_27.0M</td>\n",
       "      <td>3.854408</td>\n",
       "      <td>3.852517</td>\n",
       "      <td>3.856340</td>\n",
       "      <td>26992640</td>\n",
       "      <td>78222511308800</td>\n",
       "      <td>1846016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mnvq6sr0</td>\n",
       "      <td>model_1.8M_tokens_13.5M</td>\n",
       "      <td>4.683319</td>\n",
       "      <td>4.681561</td>\n",
       "      <td>4.685089</td>\n",
       "      <td>13492224</td>\n",
       "      <td>39099385774080</td>\n",
       "      <td>1846016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0dv6edrz</td>\n",
       "      <td>model_1.8M_tokens_6.8M</td>\n",
       "      <td>5.147617</td>\n",
       "      <td>5.146087</td>\n",
       "      <td>5.149212</td>\n",
       "      <td>6742016</td>\n",
       "      <td>19537823006720</td>\n",
       "      <td>1846016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      run_id                   run_name  val_loss  val_loss_ci_lower  \\\n",
       "0   xl2usnof   model_6.8M_tokens_120.0M  0.032641           0.032532   \n",
       "1   vwci9mym    model_6.8M_tokens_60.0M  0.044834           0.044665   \n",
       "2   agrcb37f    model_6.8M_tokens_30.0M  0.252477           0.251721   \n",
       "3   2zf1ghwn   model_6.8M_tokens_240.0M  0.028772           0.028660   \n",
       "4   h09i8eaz   model_6.8M_tokens_480.0M  0.025872           0.025769   \n",
       "5   buol3dn1  model_13.8M_tokens_960.0M  0.021329           0.021235   \n",
       "6   bh0nkk09  model_13.8M_tokens_480.0M  0.023663           0.023562   \n",
       "7   i8ikrs6s  model_13.8M_tokens_240.0M  0.026442           0.026337   \n",
       "8   g7yc8q47  model_13.8M_tokens_120.0M  0.029614           0.029507   \n",
       "9   27l1qho0   model_13.8M_tokens_60.0M  0.034794           0.034669   \n",
       "10  i052nchb   model_1.8M_tokens_108.0M  0.168284           0.167679   \n",
       "11  inzap0mx    model_1.8M_tokens_54.0M  1.129691           1.127677   \n",
       "12  1n8ous25    model_1.8M_tokens_27.0M  3.854408           3.852517   \n",
       "13  mnvq6sr0    model_1.8M_tokens_13.5M  4.683319           4.681561   \n",
       "14  0dv6edrz     model_1.8M_tokens_6.8M  5.147617           5.146087   \n",
       "\n",
       "    val_loss_ci_upper tokens_seen            compute    params  \n",
       "0            0.032760   119996416    979201473642496   6840320  \n",
       "1            0.044998    59998208    489600736821248   6840320  \n",
       "2            0.253218    29999104    244800368410624   6840320  \n",
       "3            0.028878   239992832   1958402947284992   6840320  \n",
       "4            0.025980   479993856   3916872743387136   6840320  \n",
       "5            0.021425   959995904  14015632999710720  13797376  \n",
       "6            0.023761   479993856   7007756699566080  13797376  \n",
       "7            0.026546   239992832   3503818549493760  13797376  \n",
       "8            0.029730   119996416   1751909274746880  13797376  \n",
       "9            0.034935    59998208    875954637373440  13797376  \n",
       "10           0.168922   107995136    312961264517120   1846016  \n",
       "11           1.131667    53993472    156468762378240   1846016  \n",
       "12           3.856340    26992640     78222511308800   1846016  \n",
       "13           4.685089    13492224     39099385774080   1846016  \n",
       "14           5.149212     6742016     19537823006720   1846016  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity=\"miki-and-tml\"\n",
    "project=\"scaling-tests\"\n",
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(f\"{entity}/{project}\")\n",
    "\n",
    "dtypes = {\n",
    "    'run_id': str, # or 'object'\n",
    "    'val_loss': float, # or 'float64'\n",
    "    'val_loss_ci_lower': float,\n",
    "    'val_loss_ci_upper': float,\n",
    "    'tokens_seen': int, # or 'int64'\n",
    "    'compute': float,\n",
    "    'params': int\n",
    "}\n",
    "\n",
    "# 2. Create the empty DataFrame, using keys from dtypes as columns\n",
    "runs_df = pd.DataFrame(columns=dtypes.keys())\n",
    "\n",
    "for run in runs:\n",
    "    summary = run.summary._json_dict\n",
    "    \n",
    "    # Extract final logged values, defaulting to None if not available\n",
    "    row = {\n",
    "        'run_id': run.id,\n",
    "        'run_name': run.name,\n",
    "        'val_loss': summary.get('val_loss'),\n",
    "        'val_loss_ci_lower': summary.get('val_loss_ci_lower'),\n",
    "        'val_loss_ci_upper': summary.get('val_loss_ci_upper'),\n",
    "        'tokens_seen': summary.get('tokens_seen'),\n",
    "        'compute': summary.get('compute'),\n",
    "        'params': summary.get('params'),\n",
    "    }\n",
    "    # Insert at the beginning - index 0\n",
    "    runs_df = pd.concat([pd.DataFrame([row]), runs_df]).reset_index(drop=True)\n",
    "\n",
    "runs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a72e42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  15  runs\n",
      "There are  15  finished runs\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \", len(runs_df), \" runs\")\n",
    "runs_df_finished = runs_df[runs_df['params'].notnull()]\n",
    "print(\"There are \", len(runs_df_finished), \" finished runs\")\n",
    "runs_df = runs_df_finished\n",
    "runs_df = runs_df.astype(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97904ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\h'\n",
      "/tmp/ipykernel_772450/3992794923.py:6: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  \\hat{L}(N, D) \\triangleq E+\\frac{A}{N^\\alpha}+\\frac{B}{D^\\beta}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.002642521169036627\n",
      "Epoch 100, Loss: 0.0012824843870475888\n",
      "Epoch 200, Loss: 0.0011908862506970763\n",
      "Epoch 300, Loss: 0.0011416436173021793\n",
      "Epoch 400, Loss: 0.001081848400644958\n",
      "Epoch 500, Loss: 0.0011192939709872007\n",
      "Epoch 600, Loss: 0.0009848072659224272\n",
      "Epoch 700, Loss: 0.0009640994830988348\n",
      "Epoch 800, Loss: 0.0009265423868782818\n",
      "Epoch 900, Loss: 0.0008907653973437846\n",
      "Epoch 1000, Loss: 0.0008774850284680724\n",
      "Epoch 1100, Loss: 0.000845086935441941\n",
      "Epoch 1200, Loss: 0.0008061127737164497\n",
      "Epoch 1300, Loss: 0.0007821861072443426\n",
      "Epoch 1400, Loss: 0.0007560413214378059\n",
      "Epoch 1500, Loss: 0.0007248680922202766\n",
      "Epoch 1600, Loss: 0.0007083348464220762\n",
      "Epoch 1700, Loss: 0.0006841178983449936\n",
      "Epoch 1800, Loss: 0.0006829246995039284\n",
      "Epoch 1900, Loss: 0.0006677730707451701\n",
      "Epoch 2000, Loss: 0.0006668615387752652\n",
      "Epoch 2100, Loss: 0.0006651955191046\n",
      "Epoch 2200, Loss: 0.0006743497797288001\n",
      "Epoch 2300, Loss: 0.0006788269383832812\n",
      "Epoch 2400, Loss: 0.0006664912798441947\n",
      "Epoch 2500, Loss: 0.0006593359867110848\n",
      "Epoch 2600, Loss: 0.0006693446193821728\n",
      "Epoch 2700, Loss: 0.0006504975026473403\n",
      "Epoch 2800, Loss: 0.0006520090973936021\n",
      "Epoch 2900, Loss: 0.0006629431736655533\n",
      "Epoch 3000, Loss: 0.0006686650449410081\n",
      "Epoch 3100, Loss: 0.0006636733305640519\n",
      "Epoch 3200, Loss: 0.0006573432474397123\n",
      "Epoch 3300, Loss: 0.0006705982377752662\n",
      "Epoch 3400, Loss: 0.000664927763864398\n",
      "Epoch 3500, Loss: 0.0006625782116316259\n",
      "Epoch 3600, Loss: 0.0006627360708080232\n",
      "Epoch 3700, Loss: 0.0006582534406334162\n",
      "Epoch 3800, Loss: 0.0006579247419722378\n",
      "Epoch 3900, Loss: 0.0006705432315357029\n",
      "Epoch 4000, Loss: 0.0006542357732541859\n",
      "Epoch 4100, Loss: 0.0006894461694173515\n",
      "Epoch 4200, Loss: 0.0006522588082589209\n",
      "Epoch 4300, Loss: 0.000661419820971787\n",
      "Epoch 4400, Loss: 0.0006518309819512069\n",
      "Epoch 4500, Loss: 0.0006715388735756278\n",
      "Epoch 4600, Loss: 0.0006557395681738853\n",
      "Epoch 4700, Loss: 0.0006540671456605196\n",
      "Epoch 4800, Loss: 0.0006651232833974063\n",
      "Epoch 4900, Loss: 0.0006762169650755823\n",
      "Epoch 5000, Loss: 0.0006861527217552066\n",
      "Epoch 5100, Loss: 0.0006577623425982893\n",
      "Epoch 5200, Loss: 0.0006642793887294829\n",
      "Epoch 5300, Loss: 0.0006540750619024038\n",
      "Epoch 5400, Loss: 0.0006544736097566783\n",
      "Epoch 5500, Loss: 0.000654222269076854\n",
      "Epoch 5600, Loss: 0.0006575361476279795\n",
      "Epoch 5700, Loss: 0.0006561334594152868\n",
      "Epoch 5800, Loss: 0.000661877915263176\n",
      "Epoch 5900, Loss: 0.0006555140716955066\n",
      "Epoch 6000, Loss: 0.0006648600101470947\n",
      "Epoch 6100, Loss: 0.0006540837930515409\n",
      "Epoch 6200, Loss: 0.0006545203505083919\n",
      "Epoch 6300, Loss: 0.0006948734517209232\n",
      "Epoch 6400, Loss: 0.0006625973037444055\n",
      "Epoch 6500, Loss: 0.0006580805638805032\n",
      "Epoch 6600, Loss: 0.0006651459261775017\n",
      "Epoch 6700, Loss: 0.0006572933634743094\n",
      "Epoch 6800, Loss: 0.000659902230836451\n",
      "Epoch 6900, Loss: 0.0006523260963149369\n",
      "Epoch 7000, Loss: 0.0006533010746352375\n",
      "Epoch 7100, Loss: 0.0006614277954213321\n",
      "Epoch 7200, Loss: 0.0006524658529087901\n",
      "Epoch 7300, Loss: 0.0006601913482882082\n",
      "Epoch 7400, Loss: 0.0006547395023517311\n",
      "Epoch 7500, Loss: 0.000662023841869086\n",
      "Epoch 7600, Loss: 0.0006583270151168108\n",
      "Epoch 7700, Loss: 0.0006744323181919754\n",
      "Epoch 7800, Loss: 0.0006625900859944522\n",
      "Epoch 7900, Loss: 0.0006571170524694026\n",
      "Epoch 8000, Loss: 0.0006704599363729358\n",
      "Epoch 8100, Loss: 0.0006519344751723111\n",
      "Epoch 8200, Loss: 0.0006585288792848587\n",
      "Epoch 8300, Loss: 0.0006685311673209071\n",
      "Epoch 8400, Loss: 0.0006541761104017496\n",
      "Epoch 8500, Loss: 0.0006691909511573613\n",
      "Epoch 8600, Loss: 0.0006565951043739915\n",
      "Epoch 8700, Loss: 0.0006715053459629416\n",
      "Epoch 8800, Loss: 0.0006520220777019858\n",
      "Epoch 8900, Loss: 0.0006604538066312671\n",
      "Epoch 9000, Loss: 0.0006597485626116395\n",
      "Epoch 9100, Loss: 0.0006657364428974688\n",
      "Epoch 9200, Loss: 0.0006613220321014524\n",
      "Epoch 9300, Loss: 0.0006508694496005774\n",
      "Epoch 9400, Loss: 0.0006631069700233638\n",
      "Epoch 9500, Loss: 0.0006532378029078245\n",
      "Epoch 9600, Loss: 0.0006530507816933095\n",
      "Epoch 9700, Loss: 0.0006586653762497008\n",
      "Epoch 9800, Loss: 0.0006577863241545856\n",
      "Epoch 9900, Loss: 0.0006656873738393188\n",
      "Epoch 10000, Loss: 0.0006569736870005727\n",
      "Epoch 10100, Loss: 0.0006598365143872797\n",
      "Epoch 10200, Loss: 0.0006541174370795488\n",
      "Epoch 10300, Loss: 0.0006561005138792098\n",
      "Epoch 10400, Loss: 0.0006540115573443472\n",
      "Epoch 10500, Loss: 0.0006582523928955197\n",
      "Epoch 10600, Loss: 0.0006513781845569611\n",
      "Epoch 10700, Loss: 0.0006585699738934636\n",
      "Epoch 10800, Loss: 0.0006583418580703437\n",
      "Epoch 10900, Loss: 0.000692107540089637\n",
      "Epoch 11000, Loss: 0.0006681195227429271\n",
      "Epoch 11100, Loss: 0.000656462274491787\n",
      "Epoch 11200, Loss: 0.0006556183798238635\n",
      "Epoch 11300, Loss: 0.000651490525342524\n",
      "Epoch 11400, Loss: 0.0006596339517273009\n",
      "Epoch 11500, Loss: 0.00066798843909055\n",
      "Epoch 11600, Loss: 0.0006556294392794371\n",
      "Epoch 11700, Loss: 0.0006569886463694274\n",
      "Epoch 11800, Loss: 0.0006592129939235747\n",
      "Epoch 11900, Loss: 0.0006638673366978765\n",
      "Epoch 12000, Loss: 0.0006655521574430168\n",
      "Epoch 12100, Loss: 0.000668915337882936\n",
      "Epoch 12200, Loss: 0.0006605613743886352\n",
      "Epoch 12300, Loss: 0.0006611477583646774\n",
      "Epoch 12400, Loss: 0.0006897125858813524\n",
      "Epoch 12500, Loss: 0.0006516244611702859\n",
      "Epoch 12600, Loss: 0.0006571534322574735\n",
      "Epoch 12700, Loss: 0.0006588742253370583\n",
      "Epoch 12800, Loss: 0.000656401040032506\n",
      "Epoch 12900, Loss: 0.0006578514003194869\n",
      "Epoch 13000, Loss: 0.0006582392961718142\n",
      "Epoch 13100, Loss: 0.0006601334316655993\n",
      "Epoch 13200, Loss: 0.000654323142953217\n",
      "Epoch 13300, Loss: 0.0006629041163250804\n",
      "Epoch 13400, Loss: 0.0006531605031341314\n",
      "Epoch 13500, Loss: 0.0006578905158676207\n",
      "Epoch 13600, Loss: 0.0006617432227358222\n",
      "Epoch 13700, Loss: 0.0006692743627354503\n",
      "Epoch 13800, Loss: 0.0006625857786275446\n",
      "Epoch 13900, Loss: 0.0006565583171322942\n",
      "Epoch 14000, Loss: 0.0006556694861501455\n",
      "Epoch 14100, Loss: 0.000653160095680505\n",
      "Epoch 14200, Loss: 0.0006531400140374899\n",
      "Epoch 14300, Loss: 0.0006640834035351872\n",
      "Epoch 14400, Loss: 0.0006521405884996057\n",
      "Epoch 14500, Loss: 0.0006528981029987335\n",
      "Epoch 14600, Loss: 0.0006581883644685149\n",
      "Epoch 14700, Loss: 0.000657368334941566\n",
      "Epoch 14800, Loss: 0.0006545486394315958\n",
      "Epoch 14900, Loss: 0.0006581616471521556\n",
      "Epoch 15000, Loss: 0.0006545361829921603\n",
      "Epoch 15100, Loss: 0.0006585693336091936\n",
      "Epoch 15200, Loss: 0.0006508516380563378\n",
      "Epoch 15300, Loss: 0.0006526730721816421\n",
      "Epoch 15400, Loss: 0.000656727934256196\n",
      "Epoch 15500, Loss: 0.0006548312958329916\n",
      "Epoch 15600, Loss: 0.0006650541326962411\n",
      "Epoch 15700, Loss: 0.0006665533292107284\n",
      "Epoch 15800, Loss: 0.0006582157220691442\n",
      "Epoch 15900, Loss: 0.0006516907596960664\n",
      "Epoch 16000, Loss: 0.0006513886619359255\n",
      "Epoch 16100, Loss: 0.0006530395476147532\n",
      "Epoch 16200, Loss: 0.0006547292578034103\n",
      "Epoch 16300, Loss: 0.000655834679491818\n",
      "Epoch 16400, Loss: 0.0006524520576931536\n",
      "Epoch 16500, Loss: 0.0006573969731107354\n",
      "Epoch 16600, Loss: 0.0006619287887588143\n",
      "Epoch 16700, Loss: 0.0006617522449232638\n",
      "Epoch 16800, Loss: 0.000656234857160598\n",
      "Epoch 16900, Loss: 0.0006569844554178417\n",
      "Epoch 17000, Loss: 0.0006629166309721768\n",
      "Epoch 17100, Loss: 0.0006652715965174139\n",
      "Epoch 17200, Loss: 0.0006593788275495172\n",
      "Epoch 17300, Loss: 0.0006585145019926131\n",
      "Epoch 17400, Loss: 0.0006529045640490949\n",
      "Epoch 17500, Loss: 0.0006569266552105546\n",
      "Epoch 17600, Loss: 0.0006548511446453631\n",
      "Epoch 17700, Loss: 0.0006557604065164924\n",
      "Epoch 17800, Loss: 0.0006650572177022696\n",
      "Epoch 17900, Loss: 0.0006527745281346142\n",
      "Epoch 18000, Loss: 0.0006517763831652701\n",
      "Epoch 18100, Loss: 0.0006604755762964487\n",
      "Epoch 18200, Loss: 0.0006582426722161472\n",
      "Epoch 18300, Loss: 0.0006611238932237029\n",
      "Epoch 18400, Loss: 0.0006507326033897698\n",
      "Epoch 18500, Loss: 0.0006613514851778746\n",
      "Epoch 18600, Loss: 0.0006570544792339206\n",
      "Epoch 18700, Loss: 0.0006562155322171748\n",
      "Epoch 18800, Loss: 0.0006615410093218088\n",
      "Epoch 18900, Loss: 0.000654824951197952\n",
      "Epoch 19000, Loss: 0.0006601116037927568\n",
      "Epoch 19100, Loss: 0.0006624867673963308\n",
      "Epoch 19200, Loss: 0.000652611255645752\n",
      "Epoch 19300, Loss: 0.0006571805570274591\n",
      "Epoch 19400, Loss: 0.0006617065519094467\n",
      "Epoch 19500, Loss: 0.0006627902621403337\n",
      "Epoch 19600, Loss: 0.0006650641444139183\n",
      "Epoch 19700, Loss: 0.0006618181942030787\n",
      "Epoch 19800, Loss: 0.0006525683565996587\n",
      "Epoch 19900, Loss: 0.000654051371384412\n",
      "Epoch 20000, Loss: 0.0006540581816807389\n",
      "Epoch 20100, Loss: 0.0006650388240814209\n",
      "Epoch 20200, Loss: 0.0006551162805408239\n",
      "Epoch 20300, Loss: 0.0006554838619194925\n",
      "Epoch 20400, Loss: 0.0006617767503485084\n",
      "Epoch 20500, Loss: 0.0006637423648498952\n",
      "Epoch 20600, Loss: 0.0006594544975087047\n",
      "Epoch 20700, Loss: 0.0006511714309453964\n",
      "Epoch 20800, Loss: 0.0006536672590300441\n",
      "Epoch 20900, Loss: 0.0006512123509310186\n",
      "Epoch 21000, Loss: 0.0006553120911121368\n",
      "Epoch 21100, Loss: 0.000657328637316823\n",
      "Epoch 21200, Loss: 0.0006548116216436028\n",
      "Epoch 21300, Loss: 0.0006509351660497487\n",
      "Epoch 21400, Loss: 0.0006558036548085511\n",
      "Epoch 21500, Loss: 0.0006565753719769418\n",
      "Epoch 21600, Loss: 0.0006547053926624358\n",
      "Epoch 21700, Loss: 0.0006585512310266495\n",
      "Epoch 21800, Loss: 0.0006719647790305316\n",
      "Epoch 21900, Loss: 0.0006540024769492447\n",
      "Epoch 22000, Loss: 0.0006533283740282059\n",
      "Epoch 22100, Loss: 0.0006514825508929789\n",
      "Epoch 22200, Loss: 0.0006615922902710736\n",
      "Epoch 22300, Loss: 0.0006512631080113351\n",
      "Epoch 22400, Loss: 0.0006509762024506927\n",
      "Epoch 22500, Loss: 0.0006526046781800687\n",
      "Epoch 22600, Loss: 0.0006584456423297524\n",
      "Epoch 22700, Loss: 0.0006647297414019704\n",
      "Epoch 22800, Loss: 0.0006512617692351341\n",
      "Epoch 22900, Loss: 0.0006506272475235164\n",
      "Epoch 23000, Loss: 0.0006507763755507767\n",
      "Epoch 23100, Loss: 0.0006540894974023104\n",
      "Epoch 23200, Loss: 0.000660070450976491\n",
      "Epoch 23300, Loss: 0.0006541737238876522\n",
      "Epoch 23400, Loss: 0.0006632155855186284\n",
      "Epoch 23500, Loss: 0.000658886565361172\n",
      "Epoch 23600, Loss: 0.0006798768881708384\n",
      "Epoch 23700, Loss: 0.0006529291858896613\n",
      "Epoch 23800, Loss: 0.0006557918386533856\n",
      "Epoch 23900, Loss: 0.0006514887791126966\n",
      "Epoch 24000, Loss: 0.0006663057720288634\n",
      "Epoch 24100, Loss: 0.0006575463339686394\n",
      "Epoch 24200, Loss: 0.0006644364329986274\n",
      "Epoch 24300, Loss: 0.0006524334894493222\n",
      "Epoch 24400, Loss: 0.0006548629025928676\n",
      "Epoch 24500, Loss: 0.0006560191395692527\n",
      "Epoch 24600, Loss: 0.0006621296633966267\n",
      "Epoch 24700, Loss: 0.000654748931992799\n",
      "Epoch 24800, Loss: 0.0006529281963594258\n",
      "Epoch 24900, Loss: 0.0006576718878932297\n",
      "Epoch 25000, Loss: 0.0006581368506886065\n",
      "Epoch 25100, Loss: 0.0006518308655358851\n",
      "Epoch 25200, Loss: 0.0006590759148821235\n",
      "Epoch 25300, Loss: 0.0006552176200784743\n",
      "Epoch 25400, Loss: 0.0006704856059513986\n",
      "Epoch 25500, Loss: 0.0006597661995328963\n",
      "Epoch 25600, Loss: 0.0006519726011902094\n",
      "Epoch 25700, Loss: 0.0006547757657244802\n",
      "Epoch 25800, Loss: 0.0006534535204991698\n",
      "Epoch 25900, Loss: 0.0006590039702132344\n",
      "Epoch 26000, Loss: 0.0006557143060490489\n",
      "Epoch 26100, Loss: 0.0006544717471115291\n",
      "Epoch 26200, Loss: 0.0006516163120977581\n",
      "Epoch 26300, Loss: 0.0006598234758712351\n",
      "Epoch 26400, Loss: 0.0006522829062305391\n",
      "Epoch 26500, Loss: 0.0006556359003297985\n",
      "Epoch 26600, Loss: 0.0006534884450957179\n",
      "Epoch 26700, Loss: 0.0006538248271681368\n",
      "Epoch 26800, Loss: 0.0006580608314834535\n",
      "Epoch 26900, Loss: 0.0006563153583556414\n",
      "Epoch 27000, Loss: 0.0006554593564942479\n",
      "Epoch 27100, Loss: 0.0006527177756652236\n",
      "Epoch 27200, Loss: 0.0006554948049597442\n",
      "Epoch 27300, Loss: 0.0006539084133692086\n",
      "Epoch 27400, Loss: 0.0006522383773699403\n",
      "Epoch 27500, Loss: 0.0006605276139453053\n",
      "Epoch 27600, Loss: 0.0006526329671032727\n",
      "Epoch 27700, Loss: 0.0006558119785040617\n",
      "Epoch 27800, Loss: 0.0006573938298970461\n",
      "Epoch 27900, Loss: 0.0006527086952701211\n",
      "Epoch 28000, Loss: 0.0006597863393835723\n",
      "Epoch 28100, Loss: 0.0006509545492008328\n",
      "Epoch 28200, Loss: 0.0006599824992008507\n",
      "Epoch 28300, Loss: 0.0006546287331730127\n",
      "Epoch 28400, Loss: 0.0006514809210784733\n",
      "Epoch 28500, Loss: 0.000657321885228157\n",
      "Epoch 28600, Loss: 0.0006522234180010855\n",
      "Epoch 28700, Loss: 0.0006518392474390566\n",
      "Epoch 28800, Loss: 0.0006561081972904503\n",
      "Epoch 28900, Loss: 0.0006549822865054011\n",
      "Epoch 29000, Loss: 0.0006521469331346452\n",
      "Epoch 29100, Loss: 0.0006542318733409047\n",
      "Epoch 29200, Loss: 0.0006607291870750487\n",
      "Epoch 29300, Loss: 0.0006591490237042308\n",
      "Epoch 29400, Loss: 0.0006540438043884933\n",
      "Epoch 29500, Loss: 0.000657307798974216\n",
      "Epoch 29600, Loss: 0.0006518122390843928\n",
      "Epoch 29700, Loss: 0.000652302464004606\n",
      "Epoch 29800, Loss: 0.0006522175972349942\n",
      "Epoch 29900, Loss: 0.0006511316751129925\n",
      "Epoch 30000, Loss: 0.0006521455361507833\n",
      "Epoch 30100, Loss: 0.0006540484027937055\n",
      "Epoch 30200, Loss: 0.0006588873220607638\n",
      "Epoch 30300, Loss: 0.0006554885185323656\n",
      "Epoch 30400, Loss: 0.0006523964693769813\n",
      "Epoch 30500, Loss: 0.0006525717326439917\n",
      "Epoch 30600, Loss: 0.0006538155721500516\n",
      "Epoch 30700, Loss: 0.0006525542703457177\n",
      "Epoch 30800, Loss: 0.0006515504792332649\n",
      "Epoch 30900, Loss: 0.0006534541607834399\n",
      "Epoch 31000, Loss: 0.0006511080428026617\n",
      "Epoch 31100, Loss: 0.0006563409115187824\n",
      "Epoch 31200, Loss: 0.000655870942864567\n",
      "Epoch 31300, Loss: 0.00065470120171085\n",
      "Epoch 31400, Loss: 0.0006512578693218529\n",
      "Epoch 31500, Loss: 0.000657931377645582\n",
      "Epoch 31600, Loss: 0.0006638009217567742\n",
      "Epoch 31700, Loss: 0.0006604765658266842\n",
      "Epoch 31800, Loss: 0.0006528413505293429\n",
      "Epoch 31900, Loss: 0.0006535472930409014\n",
      "Epoch 32000, Loss: 0.0006508980295620859\n",
      "Epoch 32100, Loss: 0.0006512407562695444\n",
      "Epoch 32200, Loss: 0.000654080999083817\n",
      "Epoch 32300, Loss: 0.0006517263245768845\n",
      "Epoch 32400, Loss: 0.0006581664201803505\n",
      "Epoch 32500, Loss: 0.0006518147420138121\n",
      "Epoch 32600, Loss: 0.000652457179967314\n",
      "Epoch 32700, Loss: 0.0006533262203447521\n",
      "Epoch 32800, Loss: 0.000650547503028065\n",
      "Epoch 32900, Loss: 0.0006545513751916587\n",
      "Epoch 33000, Loss: 0.0006551586557179689\n",
      "Epoch 33100, Loss: 0.0006527305813506246\n",
      "Epoch 33200, Loss: 0.0006538763409480453\n",
      "Epoch 33300, Loss: 0.0006518774316646159\n",
      "Epoch 33400, Loss: 0.0006547351949848235\n",
      "Epoch 33500, Loss: 0.0006536088767461479\n",
      "Epoch 33600, Loss: 0.0006575572770088911\n",
      "Epoch 33700, Loss: 0.0006552154663950205\n",
      "Epoch 33800, Loss: 0.0006536273285746574\n",
      "Epoch 33900, Loss: 0.0006514396518468857\n",
      "Epoch 34000, Loss: 0.000652703398372978\n",
      "Epoch 34100, Loss: 0.0006558492314070463\n",
      "Epoch 34200, Loss: 0.0006516764406114817\n",
      "Epoch 34300, Loss: 0.0006558848544955254\n",
      "Epoch 34400, Loss: 0.0006541606853716075\n",
      "Epoch 34500, Loss: 0.0006517061265185475\n",
      "Epoch 34600, Loss: 0.0006562601192854345\n",
      "Epoch 34700, Loss: 0.0006558296154253185\n",
      "Epoch 34800, Loss: 0.0006513989064842463\n",
      "Epoch 34900, Loss: 0.0006524056079797447\n",
      "Epoch 35000, Loss: 0.0006518919835798442\n",
      "Epoch 35100, Loss: 0.0006526376237161458\n",
      "Epoch 35200, Loss: 0.0006508054793812335\n",
      "Epoch 35300, Loss: 0.0006525448989123106\n",
      "Epoch 35400, Loss: 0.000650560250505805\n",
      "Epoch 35500, Loss: 0.0006533273844979703\n",
      "Epoch 35600, Loss: 0.0006520485621877015\n",
      "Epoch 35700, Loss: 0.0006506834761239588\n",
      "Epoch 35800, Loss: 0.0006514750421047211\n",
      "Epoch 35900, Loss: 0.0006553722778335214\n",
      "Epoch 36000, Loss: 0.0006524526979774237\n",
      "Epoch 36100, Loss: 0.0006528043886646628\n",
      "Epoch 36200, Loss: 0.000653905444778502\n",
      "Epoch 36300, Loss: 0.0006554509163834155\n",
      "Epoch 36400, Loss: 0.0006540046888403594\n",
      "Epoch 36500, Loss: 0.0006508106598630548\n",
      "Epoch 36600, Loss: 0.000658351113088429\n",
      "Epoch 36700, Loss: 0.0006554326391778886\n",
      "Epoch 36800, Loss: 0.0006515161949209869\n",
      "Epoch 36900, Loss: 0.0006523338961414993\n",
      "Epoch 37000, Loss: 0.0006519662565551698\n",
      "Epoch 37100, Loss: 0.0006513925036415458\n",
      "Epoch 37200, Loss: 0.0006518860463984311\n",
      "Epoch 37300, Loss: 0.0006524223135784268\n",
      "Epoch 37400, Loss: 0.0006516091525554657\n",
      "Epoch 37500, Loss: 0.0006512698601000011\n",
      "Epoch 37600, Loss: 0.0006532721454277635\n",
      "Epoch 37700, Loss: 0.0006513436674140394\n",
      "Epoch 37800, Loss: 0.000652034766972065\n",
      "Epoch 37900, Loss: 0.0006535054999403656\n",
      "Epoch 38000, Loss: 0.0006543567287735641\n",
      "Epoch 38100, Loss: 0.0006507145590148866\n",
      "Epoch 38200, Loss: 0.0006516307475976646\n",
      "Epoch 38300, Loss: 0.0006533356499858201\n",
      "Epoch 38400, Loss: 0.0006513079861178994\n",
      "Epoch 38500, Loss: 0.0006510091479867697\n",
      "Epoch 38600, Loss: 0.0006557165761478245\n",
      "Epoch 38700, Loss: 0.0006527249934151769\n",
      "Epoch 38800, Loss: 0.0006517855799756944\n",
      "Epoch 38900, Loss: 0.0006515883724205196\n",
      "Epoch 39000, Loss: 0.0006508427322842181\n",
      "Epoch 39100, Loss: 0.0006516814464703202\n",
      "Epoch 39200, Loss: 0.0006526996148750186\n",
      "Epoch 39300, Loss: 0.0006538022425957024\n",
      "Epoch 39400, Loss: 0.0006508428486995399\n",
      "Epoch 39500, Loss: 0.0006519734160974622\n",
      "Epoch 39600, Loss: 0.0006521149771288037\n",
      "Epoch 39700, Loss: 0.0006512480322271585\n",
      "Epoch 39800, Loss: 0.00065046944655478\n",
      "Epoch 39900, Loss: 0.0006516994908452034\n",
      "Epoch 40000, Loss: 0.0006525898352265358\n",
      "Epoch 40100, Loss: 0.0006521028699353337\n",
      "Epoch 40200, Loss: 0.0006520812166854739\n",
      "Epoch 40300, Loss: 0.0006506738718599081\n",
      "Epoch 40400, Loss: 0.0006505157216452062\n",
      "Epoch 40500, Loss: 0.0006521050236187875\n",
      "Epoch 40600, Loss: 0.0006517450092360377\n",
      "Epoch 40700, Loss: 0.0006525602657347918\n",
      "Epoch 40800, Loss: 0.0006514419219456613\n",
      "Epoch 40900, Loss: 0.0006511608953587711\n",
      "Epoch 41000, Loss: 0.0006511164247058332\n",
      "Epoch 41100, Loss: 0.0006514956476166844\n",
      "Epoch 41200, Loss: 0.0006511224200949073\n",
      "Epoch 41300, Loss: 0.0006517847650684416\n",
      "Epoch 41400, Loss: 0.0006506558856926858\n",
      "Epoch 41500, Loss: 0.0006508186343125999\n",
      "Epoch 41600, Loss: 0.0006508984952233732\n",
      "Epoch 41700, Loss: 0.0006512999534606934\n",
      "Epoch 41800, Loss: 0.0006510381936095655\n",
      "Epoch 41900, Loss: 0.0006509619997814298\n",
      "Epoch 42000, Loss: 0.0006508848164230585\n",
      "Epoch 42100, Loss: 0.000651108450256288\n",
      "Epoch 42200, Loss: 0.0006507314392365515\n",
      "Epoch 42300, Loss: 0.0006506654317490757\n",
      "Epoch 42400, Loss: 0.0006504325428977609\n",
      "Epoch 42500, Loss: 0.0006522209732793272\n",
      "Epoch 42600, Loss: 0.0006506278878077865\n",
      "Epoch 42700, Loss: 0.000651514099445194\n",
      "Epoch 42800, Loss: 0.0006503735203295946\n",
      "Epoch 42900, Loss: 0.0006507700309157372\n",
      "Epoch 43000, Loss: 0.0006517535075545311\n",
      "Epoch 43100, Loss: 0.0006506338831968606\n",
      "Epoch 43200, Loss: 0.0006508439546450973\n",
      "Epoch 43300, Loss: 0.0006514836568385363\n",
      "Epoch 43400, Loss: 0.0006510078092105687\n",
      "Epoch 43500, Loss: 0.0006509682862088084\n",
      "Epoch 43600, Loss: 0.000650429748930037\n",
      "Epoch 43700, Loss: 0.0006503878394141793\n",
      "Epoch 43800, Loss: 0.000651722599286586\n",
      "Epoch 43900, Loss: 0.0006504180491901934\n",
      "Epoch 44000, Loss: 0.00065044459188357\n",
      "Epoch 44100, Loss: 0.0006505466881208122\n",
      "Epoch 44200, Loss: 0.0006512531545013189\n",
      "Epoch 44300, Loss: 0.0006510848761536181\n",
      "Epoch 44400, Loss: 0.000650427711661905\n",
      "Epoch 44500, Loss: 0.0006511904648505151\n",
      "Epoch 44600, Loss: 0.000651387614198029\n",
      "Epoch 44700, Loss: 0.0006508700316771865\n",
      "Epoch 44800, Loss: 0.0006511839455924928\n",
      "Epoch 44900, Loss: 0.0006504027987830341\n",
      "Epoch 45000, Loss: 0.0006508298101834953\n",
      "Epoch 45100, Loss: 0.000650707574095577\n",
      "Epoch 45200, Loss: 0.0006505033816210926\n",
      "Epoch 45300, Loss: 0.000650789646897465\n",
      "Epoch 45400, Loss: 0.0006504370248876512\n",
      "Epoch 45500, Loss: 0.000650370551738888\n",
      "Epoch 45600, Loss: 0.000650454661808908\n",
      "Epoch 45700, Loss: 0.0006504898192360997\n",
      "Epoch 45800, Loss: 0.0006504015764221549\n",
      "Epoch 45900, Loss: 0.0006503615877591074\n",
      "Epoch 46000, Loss: 0.0006503614131361246\n",
      "Epoch 46100, Loss: 0.0006503612385131419\n",
      "Epoch 46200, Loss: 0.0006503610056824982\n",
      "Epoch 46300, Loss: 0.0006503607728518546\n",
      "Epoch 46400, Loss: 0.0006503606564365327\n",
      "Epoch 46500, Loss: 0.0006503604236058891\n",
      "Epoch 46600, Loss: 0.0006503601907752454\n",
      "Epoch 46700, Loss: 0.0006503600743599236\n",
      "Epoch 46800, Loss: 0.00065035984152928\n",
      "Epoch 46900, Loss: 0.0006503597251139581\n",
      "Epoch 47000, Loss: 0.0006503594922833145\n",
      "Epoch 47100, Loss: 0.0006503593176603317\n",
      "Epoch 47200, Loss: 0.0006503592012450099\n",
      "Epoch 47300, Loss: 0.0006503589684143662\n",
      "Epoch 47400, Loss: 0.0006503586773760617\n",
      "Epoch 47500, Loss: 0.0006503585027530789\n",
      "Epoch 47600, Loss: 0.000650358444545418\n",
      "Epoch 47700, Loss: 0.0006503582699224353\n",
      "Epoch 47800, Loss: 0.0006503581535071135\n",
      "Epoch 47900, Loss: 0.0006503579788841307\n",
      "Epoch 48000, Loss: 0.000650357804261148\n",
      "Epoch 48100, Loss: 0.0006503576296381652\n",
      "Epoch 48200, Loss: 0.000650357804261148\n",
      "Epoch 48300, Loss: 0.0006503576296381652\n",
      "Epoch 48400, Loss: 0.0006503573968075216\n",
      "Epoch 48500, Loss: 0.0006503572803921998\n",
      "Epoch 48600, Loss: 0.0006503573968075216\n",
      "Epoch 48700, Loss: 0.0006503571639768779\n",
      "Epoch 48800, Loss: 0.0006503570475615561\n",
      "Epoch 48900, Loss: 0.0006503568729385734\n",
      "Epoch 49000, Loss: 0.0006503570475615561\n",
      "Epoch 49100, Loss: 0.0006503568147309124\n",
      "Epoch 49200, Loss: 0.0006503568729385734\n",
      "Epoch 49300, Loss: 0.0006503568729385734\n",
      "Epoch 49400, Loss: 0.0006503568729385734\n",
      "Epoch 49500, Loss: 0.0006503568729385734\n",
      "Epoch 49600, Loss: 0.0006503568729385734\n",
      "Epoch 49700, Loss: 0.0006503568729385734\n",
      "Epoch 49800, Loss: 0.0006503568729385734\n",
      "Epoch 49900, Loss: 0.0006503568729385734\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class ScalingLaw(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    We model the loss as a function of number of parameters N and dataset size D as:\n",
    "    $$\n",
    "    \\hat{L}(N, D) \\triangleq E+\\frac{A}{N^\\alpha}+\\frac{B}{D^\\beta}\n",
    "    $$\n",
    "    \"\"\"\n",
    "    def __init__(self, a=0.0, b=0.0, e=0.0, alpha=0.5, beta=0.5):\n",
    "        super().__init__()\n",
    "        self.a = nn.Parameter(torch.tensor(a, dtype=torch.float32))\n",
    "        self.b = nn.Parameter(torch.tensor(b, dtype=torch.float32))\n",
    "        self.e = nn.Parameter(torch.tensor(e, dtype=torch.float32))\n",
    "        self.alpha = nn.Parameter(torch.tensor(alpha, dtype=torch.float32))\n",
    "        self.beta = nn.Parameter(torch.tensor(beta, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, N, D):\n",
    "        \"\"\"\n",
    "        N: tensor of model sizes (number of parameters)\n",
    "        D: tensor of dataset sizes\n",
    "        returns: predicted loss (same shape as N and D)\n",
    "        \"\"\"\n",
    "        # logN = torch.log(N)\n",
    "        # logD = torch.log(D)\n",
    "\n",
    "        # # Compute the three components\n",
    "        # x1 = self.a - self.alpha * logN\n",
    "        # x2 = self.b - self.beta * logD\n",
    "        # x3 = self.e.expand_as(x1)\n",
    "\n",
    "        # # Numerically stable log-sum-exp over the 3 components\n",
    "        # stacked = torch.stack([x1, x2, x3], dim=0)\n",
    "        # log_pred = torch.logsumexp(stacked, dim=0)\n",
    "\n",
    "        # return torch.exp(log_pred)\n",
    "        A = torch.exp(self.a)\n",
    "        B = torch.exp(self.b)\n",
    "        E = torch.exp(self.e)\n",
    "        return E + A / (N ** self.alpha) + B / (D ** self.beta)\n",
    "\n",
    "model = ScalingLaw()\n",
    "\n",
    "total_epochs = 50000\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-1, weight_decay=1e-9)\n",
    "# Set the number of epochs for a full cosine cycle (T_max)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epochs)\n",
    "\n",
    "# Convert to numeric types before creating tensors\n",
    "N = torch.tensor(pd.to_numeric(runs_df['params']).values, dtype=torch.float32)\n",
    "D = torch.tensor(pd.to_numeric(runs_df['tokens_seen']).values, dtype=torch.float32)\n",
    "L = torch.tensor(runs_df['val_loss'].values, dtype=torch.float32)\n",
    "criterion = torch.nn.HuberLoss(delta=1e-3)\n",
    "for epoch in range(total_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    L_pred = model(N, D)\n",
    "    loss = criterion(torch.log(L_pred), torch.log(L))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81755b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>pred</th>\n",
       "      <th>error_abs</th>\n",
       "      <th>error_rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>model_1.8M_tokens_27.0M</td>\n",
       "      <td>3.854408</td>\n",
       "      <td>0.310839</td>\n",
       "      <td>3.543570</td>\n",
       "      <td>0.919355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>model_1.8M_tokens_13.5M</td>\n",
       "      <td>4.683319</td>\n",
       "      <td>1.295973</td>\n",
       "      <td>3.387346</td>\n",
       "      <td>0.723279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>model_1.8M_tokens_54.0M</td>\n",
       "      <td>1.129691</td>\n",
       "      <td>0.088154</td>\n",
       "      <td>1.041536</td>\n",
       "      <td>0.921966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>model_1.8M_tokens_6.8M</td>\n",
       "      <td>5.147617</td>\n",
       "      <td>5.657372</td>\n",
       "      <td>0.509755</td>\n",
       "      <td>0.099027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>model_1.8M_tokens_108.0M</td>\n",
       "      <td>0.168284</td>\n",
       "      <td>0.037799</td>\n",
       "      <td>0.130485</td>\n",
       "      <td>0.775385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>model_13.8M_tokens_60.0M</td>\n",
       "      <td>0.034794</td>\n",
       "      <td>0.074985</td>\n",
       "      <td>0.040191</td>\n",
       "      <td>1.155118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model_6.8M_tokens_60.0M</td>\n",
       "      <td>0.044834</td>\n",
       "      <td>0.074985</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.672495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model_13.8M_tokens_120.0M</td>\n",
       "      <td>0.029614</td>\n",
       "      <td>0.034823</td>\n",
       "      <td>0.005209</td>\n",
       "      <td>0.175897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model_6.8M_tokens_240.0M</td>\n",
       "      <td>0.028772</td>\n",
       "      <td>0.025737</td>\n",
       "      <td>0.003035</td>\n",
       "      <td>0.105482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model_6.8M_tokens_480.0M</td>\n",
       "      <td>0.025872</td>\n",
       "      <td>0.023682</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.084652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model_6.8M_tokens_120.0M</td>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.034823</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.066820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model_13.8M_tokens_960.0M</td>\n",
       "      <td>0.021329</td>\n",
       "      <td>0.023217</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.088540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model_13.8M_tokens_240.0M</td>\n",
       "      <td>0.026442</td>\n",
       "      <td>0.025737</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.026672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model_6.8M_tokens_30.0M</td>\n",
       "      <td>0.252477</td>\n",
       "      <td>0.252530</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model_13.8M_tokens_480.0M</td>\n",
       "      <td>0.023663</td>\n",
       "      <td>0.023682</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     run_name  val_loss      pred  error_abs  error_rel\n",
       "12    model_1.8M_tokens_27.0M  3.854408  0.310839   3.543570   0.919355\n",
       "13    model_1.8M_tokens_13.5M  4.683319  1.295973   3.387346   0.723279\n",
       "11    model_1.8M_tokens_54.0M  1.129691  0.088154   1.041536   0.921966\n",
       "14     model_1.8M_tokens_6.8M  5.147617  5.657372   0.509755   0.099027\n",
       "10   model_1.8M_tokens_108.0M  0.168284  0.037799   0.130485   0.775385\n",
       "9    model_13.8M_tokens_60.0M  0.034794  0.074985   0.040191   1.155118\n",
       "1     model_6.8M_tokens_60.0M  0.044834  0.074985   0.030151   0.672495\n",
       "8   model_13.8M_tokens_120.0M  0.029614  0.034823   0.005209   0.175897\n",
       "3    model_6.8M_tokens_240.0M  0.028772  0.025737   0.003035   0.105482\n",
       "4    model_6.8M_tokens_480.0M  0.025872  0.023682   0.002190   0.084652\n",
       "0    model_6.8M_tokens_120.0M  0.032641  0.034823   0.002181   0.066820\n",
       "5   model_13.8M_tokens_960.0M  0.021329  0.023217   0.001888   0.088540\n",
       "7   model_13.8M_tokens_240.0M  0.026442  0.025737   0.000705   0.026672\n",
       "2     model_6.8M_tokens_30.0M  0.252477  0.252530   0.000053   0.000209\n",
       "6   model_13.8M_tokens_480.0M  0.023663  0.023682   0.000019   0.000794"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(N, D)\n",
    "runs_df['pred'] = preds.detach().numpy()\n",
    "runs_df['error_abs'] = (runs_df.pred - runs_df.val_loss).abs()\n",
    "runs_df['error_rel'] = runs_df.error_abs / runs_df.val_loss\n",
    "runs_df[['run_name', 'val_loss', 'pred', 'error_abs', 'error_rel']].sort_values(by='error_abs', ascending=False)\n",
    "# runs_df[['run_name', 'val_loss', 'pred', 'error_abs', 'error_rel']].sort_values(by='error_rel', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d11c2298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A=0.032006070017814636, B=2474230801235968.0, E=0.02308114990592003, alpha=3.169862985610962, beta=2.1442453861236572\n"
     ]
    }
   ],
   "source": [
    "A = torch.exp(model.a).item()\n",
    "B = torch.exp(model.b).item()\n",
    "E = torch.exp(model.e).item()\n",
    "alpha = model.alpha.item()\n",
    "beta = model.beta.item()\n",
    "print(f\"A={A}, B={B}, E={E}, alpha={alpha}, beta={beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66c83395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': np.float64(9.881793208823115), 'b': np.float64(21.47996702245739), 'e': np.float64(-40.44384477437448), 'alpha': np.float64(2.336459634245038), 'beta': np.float64(1.261894611433897), 'A': np.float64(19570.7857206063), 'B': np.float64(2131234065.6864407), 'E': np.float64(2.7255953996100603e-18), 'res':   message: CONVERGENCE: RELATIVE REDUCTION OF F <= FACTR*EPSMCH\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 14.033634185791016\n",
      "        x: [ 9.882e+00  2.148e+01 -4.044e+01  2.336e+00  1.262e+00]\n",
      "      nit: 32\n",
      "      jac: [-5.497e-10  1.265e-03 -3.299e-16  8.046e-09 -2.119e-01]\n",
      "     nfev: 88\n",
      "     njev: 88\n",
      " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>}\n"
     ]
    }
   ],
   "source": [
    "# jax_fit.py\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.special import logsumexp\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "N_arr = jnp.array(runs_df['params'])\n",
    "D_arr = jnp.array(runs_df['tokens_seen'])\n",
    "L_arr = jnp.array(runs_df['val_loss'])\n",
    "\n",
    "def huber(z, delta=1e-3):\n",
    "    a = jnp.abs(z)\n",
    "    return jnp.where(a <= delta, 0.5 * z * z / delta, a - 0.5 * delta)\n",
    "\n",
    "def obj_and_grad(x, N_np, D_np, L_np, delta=1e-3):\n",
    "    # x = [a, b, e, alpha, beta]\n",
    "    a, b, e, alpha, beta = x\n",
    "    # convert inputs to jnp arrays\n",
    "    N = jnp.asarray(N_np)\n",
    "    D = jnp.asarray(D_np)\n",
    "    L = jnp.asarray(L_np)\n",
    "\n",
    "    t1 = a - alpha * jnp.log(N)\n",
    "    t2 = b - beta  * jnp.log(D)\n",
    "    t3 = e\n",
    "    # log-sum-exp across the three terms (per-run)\n",
    "    # produce scalar per run then difference with log L\n",
    "    lse = logsumexp(jnp.stack([t1, t2, jnp.full_like(t1, t3)]), axis=0)\n",
    "    residual = lse - jnp.log(L)\n",
    "    losses = huber(residual, delta=delta)\n",
    "    total = jnp.sum(losses)\n",
    "    return total\n",
    "\n",
    "# jax wrapper returning value and gradient for scipy\n",
    "def value_and_grad_jax(x, N_np, D_np, L_np, delta=1e-3):\n",
    "    val = obj_and_grad(x, N_np, D_np, L_np, delta)\n",
    "    grad = jax.grad(lambda xx: obj_and_grad(xx, N_np, D_np, L_np, delta))(x)\n",
    "    return val, grad\n",
    "\n",
    "def fit_jax(N_arr, D_arr, L_arr, x0=None, delta=1e-3):\n",
    "    if x0 is None:\n",
    "        # reasonable default starting point (paper uses a grid; pick middle)\n",
    "        x0 = np.array([10.0, 10.0, 0.0, 0.5, 0.5], dtype=float)\n",
    "\n",
    "    def fun_and_grad_np(x):\n",
    "        val, grad = value_and_grad_jax(x, N_arr, D_arr, L_arr, delta)\n",
    "        return np.asarray(val, dtype=float), np.asarray(grad, dtype=float)\n",
    "\n",
    "    res = minimize(fun_and_grad_np, x0, method=\"L-BFGS-B\", jac=True)\n",
    "    a, b, e, alpha, beta = res.x\n",
    "    A, B, E = np.exp(a), np.exp(b), np.exp(e)\n",
    "    return {\"a\": a, \"b\": b, \"e\": e, \"alpha\": alpha, \"beta\": beta,\n",
    "            \"A\": A, \"B\": B, \"E\": E, \"res\": res}\n",
    "\n",
    "# Example usage:\n",
    "out = fit_jax(N_arr, D_arr, L_arr)\n",
    "print(out)\n",
    "def pred(N, D, params=out):\n",
    "    A = params['A']\n",
    "    B = params['B']\n",
    "    E = params['E']\n",
    "    alpha = params['alpha']\n",
    "    beta = params['beta']\n",
    "    return E + A / (N ** alpha) + B / (D ** beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c15e2b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>pred</th>\n",
       "      <th>pred2</th>\n",
       "      <th>better</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model_6.8M_tokens_120.0M</td>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.034823</td>\n",
       "      <td>0.136010</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model_6.8M_tokens_60.0M</td>\n",
       "      <td>0.044834</td>\n",
       "      <td>0.074985</td>\n",
       "      <td>0.326167</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model_6.8M_tokens_30.0M</td>\n",
       "      <td>0.252477</td>\n",
       "      <td>0.252530</td>\n",
       "      <td>0.782183</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model_6.8M_tokens_240.0M</td>\n",
       "      <td>0.028772</td>\n",
       "      <td>0.025737</td>\n",
       "      <td>0.056716</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model_6.8M_tokens_480.0M</td>\n",
       "      <td>0.025872</td>\n",
       "      <td>0.023682</td>\n",
       "      <td>0.023650</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model_13.8M_tokens_960.0M</td>\n",
       "      <td>0.021329</td>\n",
       "      <td>0.023217</td>\n",
       "      <td>0.009862</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model_13.8M_tokens_480.0M</td>\n",
       "      <td>0.023663</td>\n",
       "      <td>0.023682</td>\n",
       "      <td>0.023650</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model_13.8M_tokens_240.0M</td>\n",
       "      <td>0.026442</td>\n",
       "      <td>0.025737</td>\n",
       "      <td>0.056716</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model_13.8M_tokens_120.0M</td>\n",
       "      <td>0.029614</td>\n",
       "      <td>0.034823</td>\n",
       "      <td>0.136010</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>model_13.8M_tokens_60.0M</td>\n",
       "      <td>0.034794</td>\n",
       "      <td>0.074985</td>\n",
       "      <td>0.326167</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>model_1.8M_tokens_108.0M</td>\n",
       "      <td>0.168284</td>\n",
       "      <td>0.037799</td>\n",
       "      <td>0.155354</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>model_1.8M_tokens_54.0M</td>\n",
       "      <td>1.129691</td>\n",
       "      <td>0.088154</td>\n",
       "      <td>0.372590</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>model_1.8M_tokens_27.0M</td>\n",
       "      <td>3.854408</td>\n",
       "      <td>0.310839</td>\n",
       "      <td>0.893681</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>model_1.8M_tokens_13.5M</td>\n",
       "      <td>4.683319</td>\n",
       "      <td>1.295973</td>\n",
       "      <td>2.143962</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>model_1.8M_tokens_6.8M</td>\n",
       "      <td>5.147617</td>\n",
       "      <td>5.657372</td>\n",
       "      <td>5.145388</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     run_name  val_loss      pred     pred2  better\n",
       "0    model_6.8M_tokens_120.0M  0.032641  0.034823  0.136010   False\n",
       "1     model_6.8M_tokens_60.0M  0.044834  0.074985  0.326167   False\n",
       "2     model_6.8M_tokens_30.0M  0.252477  0.252530  0.782183   False\n",
       "3    model_6.8M_tokens_240.0M  0.028772  0.025737  0.056716   False\n",
       "4    model_6.8M_tokens_480.0M  0.025872  0.023682  0.023650   False\n",
       "5   model_13.8M_tokens_960.0M  0.021329  0.023217  0.009862   False\n",
       "6   model_13.8M_tokens_480.0M  0.023663  0.023682  0.023650    True\n",
       "7   model_13.8M_tokens_240.0M  0.026442  0.025737  0.056716   False\n",
       "8   model_13.8M_tokens_120.0M  0.029614  0.034823  0.136010   False\n",
       "9    model_13.8M_tokens_60.0M  0.034794  0.074985  0.326167   False\n",
       "10   model_1.8M_tokens_108.0M  0.168284  0.037799  0.155354    True\n",
       "11    model_1.8M_tokens_54.0M  1.129691  0.088154  0.372590    True\n",
       "12    model_1.8M_tokens_27.0M  3.854408  0.310839  0.893681    True\n",
       "13    model_1.8M_tokens_13.5M  4.683319  1.295973  2.143962    True\n",
       "14     model_1.8M_tokens_6.8M  5.147617  5.657372  5.145388    True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs_df['pred2'] = runs_df.apply(lambda row: pred(row['params'], row['tokens_seen']), axis=1)\n",
    "runs_df['better'] = (runs_df['error_abs'] > (runs_df['pred2'] - runs_df['val_loss']).abs())\n",
    "runs_df[['run_name', 'val_loss', 'pred', 'pred2', 'better']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af249884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
